\documentclass[titlepage]{article}


\usepackage[margin=1.5in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{pgfplots}
\usepackage{enumitem}
\usepackage{placeins}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{fancyvrb}

\author{Eddie Shim}
\title{Coursera Machine Learning Notes}




\begin{document}
	\maketitle
	
	\section{Regression}
	\subsection{Linear Regression}
	To perform a univariate linear regression, take a dataset of two variables and optimize the linear equation $y = \theta_1 x + \theta_0$. Use the least squares method which minimizes the convex parabolic equation $\sum_{i=1}^{m} (f(x) - y)^2$ , (where $(f(x)) -y)^2$ represents the residual squared). To find the minimal $\vec \theta = [\theta_0 \ \theta_1]$, calculate where the derivative of the equation equals 0. 
	
\begin{itemize}
	
	
	\item \textbf{Cost function for linear regression} (where $\vec{\theta} \in \mathbb{R}^{n+1}$, where n is the number of independent variables):
	\begin{align*}
	J(\vec\theta) = \frac{1}{2m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^2
	\end{align*}
	
	
	\item \textbf{Gradient descent} (Note: must update all $\theta _j$ simultaneously):
	\begin{align*}
	\theta _j := \theta _j - \alpha \frac{\partial}{\partial \theta _j} J(\vec\theta) 
	\end{align*}
	ex: for 2 variable case: 
	
\begin{align*}
\theta_0 &:= \theta _0 - \alpha \frac{1}{m}\sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \\
\theta _1 &:= \theta _1 - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_\theta (x^{(i)}) - y^{(i)}) x^{(i)} \\
\end{align*}
	

\item \textbf{Feature scaling}: normalizing all independent variables to an appropriate range. Purpose is to speed up gradient descent.
\begin{align*}
x_i = \frac{x_i - \mu}{\textrm{range of } x}
\end{align*}


\item Note: if $\alpha$ is too large, gradient descent may skip minimum point. However, if $\alpha$ is too little, it may take too long to converge.

\item \textbf{Normal equation} an alternative to gradient descent: 
\begin{align*}
\theta = (X^TX)^{-1}y
\end{align*}

\FloatBarrier
\begin{table} [!htbp]
	\centering
	\begin{tabular}{p{5cm}|p{6cm}}
		\textbf{Gradient Descent} & \textbf{Normal Equation}\\
		\hline
		- need to choose an $\alpha$ & - no need for $\alpha$\\
		- needs many iterations & - no need to iterate, one calculation\\
		- works well even when n is large & - need to compute $(X^TX)^{-1}$ which runs in $O(n^3)$ runtime\\
		& - slow if n is very large
	\end{tabular}
	
	\caption{Pros and Cons}
\end{table}


\newpage

\item \textbf{Vectorization}: to speed up loops.\\
e.g. Transform the following code from: 

\begin{Verbatim}[obeytabs]
for i = 1:3
   for j = 1:m
      theta(i) := theta(i) - alpha * (1/m)*(h_theta(x(j))-y(j))*x(i);
   end
end
\end{Verbatim}


into:
\begin{verbatim}
theta = theta - alpha * delta;
\end{verbatim}

where $\theta \in \mathbb{R}^{n+1},\ \alpha \in \mathbb{R},\ \delta \in \mathbb{R}^{n+1}$

\subsection{Logistic Regression}
\item A classification algorithm (not really regression, which predicts continuous variable given continuous variable)
\begin{align*}
h_\theta (x) &= g(\theta^Tx), \\
\text{where} \ g(z) &= \frac{1}{1+e^{-z}} \ \ \ (\text{sigmoid function})
\end{align*}

\begin{center}
\begin{tikzpicture}
\begin{axis}[xlabel=$z$,ylabel=$g(z)$]
\addplot[color=orange] {1/(1+exp{-x})};
\end{axis}
\end{tikzpicture}	
\end{center}

\item Nice properties:
\begin{itemize} [label = $\bullet$]
	\item {$0 \leq h_\theta (x) \leq 1$}, good for properties of a probability
	\item At $z=0$, $g(z) = 0.5$
	\item Converges to $g(z) =1$ quickly as $g$ increases, and vice versa for 0
	\item We can use these properties to output the probability that an input exists in one of two binary states (1 or 0)
\end{itemize}


\item Terminology: the probability of the output of results equaling 1:
\begin{align*}
h_\theta (x) = p(y=1 | \ x;\theta)
\end{align*}


\item Predict $y=1$ if $\theta^T x \geq 0 \Leftrightarrow \text{if} \ h_\theta(x) = g(\theta^Tx) \geq 0.5$

\item Cost Function:
\begin{align*}
J(\vec\theta) = \frac{1}{m} \sum_{i=1}^{m} \text{cost}(h_\theta (x),y)\\
\end{align*}
where cost $(h_\theta(x),y)$ = 
\[ \begin{cases} 
-log(h_\theta(x)) \ &\text{if} \  y=1\\ 
-log(1-h_\theta(x)) \ &\text{if} \  y =0 
\end{cases} \]


\begin{figure}[h]
	\begin{minipage}{.55\textwidth}
			\begin{tikzpicture}[scale=0.8]
			\begin{axis}[xlabel=$h_\theta$(x),ylabel=cost]
			\addplot[color=orange] {-ln(x)};
			\end{axis}
			\end{tikzpicture}	
	\end{minipage}
		\begin{minipage}{.01\textwidth}
			\begin{tikzpicture}[scale=0.8]
			\begin{axis}[xmin=0, xmax=1,xlabel=$h_\theta(x)$,ylabel=cost]
			\addplot[color=orange] {ln(1-x)};
			\end{axis}
			\end{tikzpicture}
				
		\end{minipage}	
\end{figure}

\item We can create a more succinct function instead of using a piecewise function:

\begin{align*} 
\text{cost}(h_\theta(x),y) &= -y*log(h_\theta(x)) - (1-y)*log(1-h_\theta(x))\\
J(\vec{\theta}) &= -\frac{1}{m}\Big(\sum_{i=1}^{m} y^{(i)}*log(h_\theta(x^{(i)})) + (1-y^{(i)})*log(1-h_\theta(x^{(i)}))\Big)
\end{align*}





\end{itemize}
\end{document}
